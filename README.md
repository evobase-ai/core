# EvoBase Framework  
*Also known as EvolvingAgent-Zero in academic literature*

**A closed-loop self-evolution framework for on-device small language models (SLMs)**   
**Assisted by**: ChatGPT and Grok  
**License**: MIT

[![DOI v1: EvolvingAgent-Zero](https://zenodo.org/badge/DOI/10.5281/zenodo.17549670.svg)](https://doi.org/10.5281/zenodo.17549670)  
[![DOI v2: EvoBase Framework](https://zenodo.org/badge/DOI/10.5281/zenodo.17555914.svg)](https://doi.org/10.5281/zenodo.17555914)

## One-Click Demo

```bash
pip install -r requirements.txt
python RUN_ME.py
```

> **Output**: Daily interaction â†’ VAMS filtering â†’ Sleep consolidation â†’ SQIA check â†’ LoRA correction â†’ State saved

### ğŸ‡¨ğŸ‡³ ä¸­å›½å¢ƒå†…ç”¨æˆ·ä¸“å±æ–¹æ¡ˆ
**æ‰€æœ‰æ¨¡å‹å‡é€šè¿‡é­”æ­ï¼ˆModelScopeï¼‰é•œåƒä¸‹è½½ï¼Œæ— éœ€ç§‘å­¦ä¸Šç½‘**ï¼š
```bash
USE_MODELSCOPE=true python RUN_ME.py  # Linux/Mac
# æˆ–
set USE_MODELSCOPE=true && python RUN_ME.py  # Windows
```
> âœ… å·²éªŒè¯å¯åœ¨æ— å¤–ç½‘ç¯å¢ƒä¸‹è¿è¡Œ  
> âœ… è¦†ç›– Qwen æ¨¡å‹ + Sentence-BERT åµŒå…¥æ¨¡å‹

---

## Core Closed Loop

```
User Interaction 
    â†’ VAMS Scoring (Value-Aligned Memory Scoring) 
    â†’ Vector Memory Store 
    â†’ Sleep Distiller (Nightly Consolidation) 
    â†’ SQIA Self-Check (Identity Anchoring) 
    â†’ LoRA Replacement 
    â†’ New Model Version
```

---

## Key Mechanisms

| Component | Formula / Logic |
|---------|-----------------|
| **VAMS** | `Score = R Ã— (0.4 + 0.3E + 0.3V)` <br> `R`: keyword-based relevance, `E`: TextBlob sentiment polarity, `V`: value-aligned keyword matching |
| **SQIA** | `Drift = 1 - cos(Gen(q), Truth(q))` <br> Uses Sentence-BERT embeddings; `if Drift > 0.15 â†’ LoRA correction` |
| **Sleep** | Keep top-k memories by VAMS score |
| **LoRA** | PEFT-based adapter on `q_proj/v_proj` layers; fine-tuned during SQIA correction |

---

## Citation

### For the **academic paper** (EvolvingAgent-Zero):
```bibtex
@misc{wang2025evolvingagent,
  author       = {Wang, Zhongren},
  title        = {EvolvingAgent-Zero: A Self-Evolving Framework for Continuous Personalization in Small-Scale Language Models},
  year         = 2025,
  month        = nov,
  doi          = {10.5281/zenodo.17549670},
  url          = {https://doi.org/10.5281/zenodo.17549670},
  note         = {First academic proposal on November 8, 2025}
}
```

### For the **open-source framework** (EvoBase):
```bibtex
@software{wang2025evobase,
  author       = {Wang, Zhongren},
  title        = {EvoBase Framework: Continuous Self-Evolution via VAMS, SQIA, and LoRA Replacement},
  year         = 2025,
  month        = nov,
  doi          = {10.5281/zenodo.17555914},
  url          = {https://github.com/evobase-ai/core},
  note         = {Open-source implementation (MIT License), first released November 10, 2025}
}
```

---

## Files

- [`EvoBase Framework.pdf`](EvoBase%20Framework.pdf) â€“ Full specification with pseudocode and diagrams  
- [`src/evobase.py`](src/evobase.py) â€“ Core engine (now with real VAMS/SQIA/LoRA)  
- [`RUN_ME.py`](RUN_ME.py) â€“ One-click demo script (supports ModelScope fallback)  
- [`requirements.txt`](requirements.txt) â€“ Dependencies (includes sentence-transformers, textblob, peft, modelscope)  

---

## Run Locally (China Optimized)

```bash
git clone https://github.com/evobase-ai/core.git
cd core
pip install -r requirements.txt
python -m textblob.download_corpora  # Required for VAMS sentiment analysis
```

### ä¸­å›½å¢ƒå†…è¿è¡Œå‘½ä»¤ï¼š
```bash
# å¯ç”¨å…¨é“¾è·¯é­”æ­é•œåƒï¼ˆæ— éœ€å¤–ç½‘ï¼‰
USE_MODELSCOPE=true python RUN_ME.py  # Linux/Mac
set USE_MODELSCOPE=true && python RUN_ME.py  # Windows
```

> **Note**: First run will download from **é­”æ­ ModelScope**:
> - Qwen2-0.5B model (~1.2GB) 
> - Sentence-BERT (`all-MiniLM-L6-v2`, ~80MB)
> **å…¨ç¨‹ä½¿ç”¨å›½å†… CDN åŠ é€Ÿï¼Œæ— éœ€ä»»ä½•ä»£ç†**

---

**Â© 2025 EvoBase Project Â· Open Source Â· MIT License**  
**All future works must cite both DOIs to respect priority.**